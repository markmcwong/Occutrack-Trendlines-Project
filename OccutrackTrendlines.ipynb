{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Idea:\n",
        "\n",
        "## Using an XGBoost Model\n",
        "\n",
        "1.   Start by collecting a dataset of labeled images, where the label is the \n",
        "direction of the gaze.\n",
        "\n",
        "2.   Extract the features (e.g. pupil_sizes, eye_corners, eyebrow_contours, faces, and eye positions) from each image with OpenCV and use them as input to the XGBoost model.\n",
        "\n",
        "3. Train the XGBoost model on the labeled data, using the extracted features as input and the gaze direction as the output label.\n",
        "\n",
        "4. Use the feature importance analysis in XGBoost along with SHAP to identify which features are most important for the prediction, so that we know which features the model is using to make its decisions.\n",
        "\n",
        "(Justification: Since XGBoost is a powerful and efficient machine learning model that is often used for classification and is particularly effective at handling structured data, providing excellent accuracy and interpretability.)\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Input:\n",
        "- pupil_sizes, eye_corners, eyebrow_contours, faces, and eye positions\n",
        "\n",
        "(**Justification** for each input feature is explained in each section later) \n",
        "\n",
        "## Output:\n",
        "- 1 out of the 9 Eye-gazing directions (Up, down, left, right, etc)\n",
        "\n",
        "(**Justification**: Predefining 9 eye gazing directions allows the classification problem to be simplified and reduces the number of classes that the model has to distinguish between. This makes it easier to train the model and achieve a high accuracy. Additionally, it may be more interpretable to humans to see the prediction as one of 9 distinct classes rather than as a set of coordinates. However, using a continuous set of coordinates could be useful if the problem requires more precision or if the classification categories are not clearly defined)\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cW9aWdFs0FQ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Procedures:\n",
        "\n",
        "\n",
        "1.   Locate and extract faces in the image\n",
        "2.   Extract eyes and eyebrows from the faces detected\n",
        "3.   Extract eye corners from the eyes\n",
        "4.   Extract pupil size from the eyes\n",
        "5.   Combine all the information we have into a feature matrix\n",
        "6.   Perform data preprocessing\n",
        "7.   Train the model\n",
        "8.   Identify and understand how each feature contributes\n",
        "\n"
      ],
      "metadata": {
        "id": "m6b6zhwHdB_V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading Data, Importing Libraries and Defining Variables"
      ],
      "metadata": {
        "id": "ZE7gWZp9Us8s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install shap\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import shap\n",
        "\n",
        "!wget \"https://github.com/opencv/opencv/raw/master/data/haarcascades/haarcascade_frontalface_default.xml\"\n",
        "!wget \"https://github.com/opencv/opencv/raw/master/data/haarcascades/haarcascade_eye.xml\"\n",
        "!wget \"https://github.com/npinto/opencv/raw/master/data/haarcascades/haarcascade_mcs_eyepair_big.xml\"\n",
        "\n",
        "prefix = \"MPIIGaze/\"\n",
        "file_path = \"test.jpg\"\n",
        "\n",
        "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
        "eye_cascade = cv2.CascadeClassifier('haarcascade_eye.xml')\n",
        "eyebrow_cascade = cv2.CascadeClassifier('haarcascade_mcs_eyepair_big.xml')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTEAlH5Ur9Rq",
        "outputId": "dad50773-dd70-42ac-f2d9-65d76c6f1af5"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting shap\n",
            "  Downloading shap-0.41.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (575 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m575.9/575.9 KB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numba in /usr/local/lib/python3.8/dist-packages (from shap) (0.56.4)\n",
            "Requirement already satisfied: tqdm>4.25.0 in /usr/local/lib/python3.8/dist-packages (from shap) (4.64.1)\n",
            "Collecting slicer==0.0.7\n",
            "  Downloading slicer-0.0.7-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.8/dist-packages (from shap) (2.2.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from shap) (1.3.5)\n",
            "Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.8/dist-packages (from shap) (23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from shap) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from shap) (1.7.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from shap) (1.0.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.8/dist-packages (from numba->shap) (6.0.0)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.8/dist-packages (from numba->shap) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from numba->shap) (57.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->shap) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->shap) (2022.7.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->shap) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->shap) (3.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->shap) (1.15.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata->numba->shap) (3.12.1)\n",
            "Installing collected packages: slicer, shap\n",
            "Successfully installed shap-0.41.0 slicer-0.0.7\n",
            "--2023-02-17 06:16:47--  https://github.com/opencv/opencv/raw/master/data/haarcascades/haarcascade_frontalface_default.xml\n",
            "Resolving github.com (github.com)... 192.30.255.112\n",
            "Connecting to github.com (github.com)|192.30.255.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_frontalface_default.xml [following]\n",
            "--2023-02-17 06:16:47--  https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_frontalface_default.xml\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 930127 (908K) [text/plain]\n",
            "Saving to: ‘haarcascade_frontalface_default.xml.3’\n",
            "\n",
            "haarcascade_frontal 100%[===================>] 908.33K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2023-02-17 06:16:47 (28.6 MB/s) - ‘haarcascade_frontalface_default.xml.3’ saved [930127/930127]\n",
            "\n",
            "--2023-02-17 06:16:47--  https://github.com/opencv/opencv/raw/master/data/haarcascades/haarcascade_eye.xml\n",
            "Resolving github.com (github.com)... 192.30.255.112\n",
            "Connecting to github.com (github.com)|192.30.255.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_eye.xml [following]\n",
            "--2023-02-17 06:16:47--  https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_eye.xml\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 341406 (333K) [text/plain]\n",
            "Saving to: ‘haarcascade_eye.xml.3’\n",
            "\n",
            "haarcascade_eye.xml 100%[===================>] 333.40K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2023-02-17 06:16:48 (14.7 MB/s) - ‘haarcascade_eye.xml.3’ saved [341406/341406]\n",
            "\n",
            "--2023-02-17 06:16:48--  https://github.com/npinto/opencv/raw/master/data/haarcascades/haarcascade_mcs_eyepair_big.xml\n",
            "Resolving github.com (github.com)... 192.30.255.112\n",
            "Connecting to github.com (github.com)|192.30.255.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/npinto/opencv/master/data/haarcascades/haarcascade_mcs_eyepair_big.xml [following]\n",
            "--2023-02-17 06:16:48--  https://raw.githubusercontent.com/npinto/opencv/master/data/haarcascades/haarcascade_mcs_eyepair_big.xml\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 358385 (350K) [text/plain]\n",
            "Saving to: ‘haarcascade_mcs_eyepair_big.xml.3’\n",
            "\n",
            "haarcascade_mcs_eye 100%[===================>] 349.99K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2023-02-17 06:16:48 (17.2 MB/s) - ‘haarcascade_mcs_eyepair_big.xml.3’ saved [358385/358385]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Image Preprocessing - Removing Noise\n",
        "\n",
        "Noise in images could be sensor noise, compression artifacts, or poor lighting conditions. Removing noise from an image can help improve its visual quality and make it more useful for further analysis.\n",
        "\n",
        "In order to remove noise from an image, we could use spatial filters like Gaussian smoothing to blur the image and reduce high-frequency noise. \n",
        "\n",
        "Another approach is to use thresholding or adaptive thresholding to isolate regions of the image that are most likely to contain the features of interest, while suppressing noise in other regions.\n",
        "\n",
        "---\n",
        "\n",
        "##**Justification**: \n",
        "- I chose Gaussian filter because it has a smoothing effect while preserving the edges of the image, while computationally efficient\n",
        "\n",
        "- I chose adaptive thresholding in this case as it would be a better approach for image segmentation and edge detection in cases where the lighting conditions are not consistent throughout the image, as it is capable of automatically determining the optimal threshold value for each local region of the image, based on the characteristics of the local pixel intensities"
      ],
      "metadata": {
        "id": "2pIX91kpnokP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def image_preprocessing(img_path):\n",
        "  # Load input image\n",
        "  img = cv2.imread(img_path)\n",
        "\n",
        "  # Convert image to grayscale\n",
        "  img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "  # Use adaptive threshold\n",
        "  cv2.adaptiveThreshold(img_gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 9, 3)\n",
        "\n",
        "  return img, img_gray\n",
        "\n",
        "img, img_gray = image_preprocessing(file_path)"
      ],
      "metadata": {
        "id": "jo_Jn1dHmDT3"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extracting Faces, Eyes, and Eyebrows\n",
        "\n",
        "- Cascade classifier is used to identify faces and eyes by computing Haar-like features, as it is fast and effective\n",
        "\n",
        "- Once the face and eye regions have been identified, a region of interest (ROI) can be defined above each eye to isolate the area where the eyebrows are most likely to be found.\n",
        "\n",
        "- In the code, I used color analysis to identify the specific regions within the ROI that correspond to the eyebrows and extract pixels with specific color values or by analyzing color histograms within the ROI.\n",
        "\n",
        "---\n",
        "### Justification of extracting eyebrows:\n",
        "Position of the eyebrows relative to the eyes can provide information about the direction of the gaze. When a person looks up or down, their eyebrows move accordingly, and these changes can be detected by tracking the position of the eyebrows.\n",
        "\n",
        "\n",
        "###Improvements I could make:\n",
        "Extracted eyebrow regions can be further processed using techniques such as edge detection or morphological operations to improve their shape and position accuracy. "
      ],
      "metadata": {
        "id": "ss0p67Urc-zd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_faces(img, img_gray):\n",
        "  face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
        "  faces = face_cascade.detectMultiScale(img, 1.3, 5)\n",
        "\n",
        "  imgs_with_faces_only = [img[y:y+h, x:x+w] for (x, y, w, h) in faces]\n",
        "  imgs_gray_with_faces_only = [img_gray[y:y+h, x:x+w] for (x, y, w, h) in faces]\n",
        "\n",
        "  return faces, imgs_with_faces_only, imgs_gray_with_faces_only\n",
        "\n",
        "faces, imgs_with_faces_only, imgs_gray_with_faces_only = extract_faces(img, img_gray)"
      ],
      "metadata": {
        "id": "VxF8rRpRtXdj"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_eyes(img):\n",
        "  eyes_roi = [img[y:y+h, x:x+w] for x, y, w, h in eye_cascade.detectMultiScale(img)]\n",
        "  return eyes_roi\n",
        "\n",
        "eye_positions = [extract_eyes(img) for img in imgs_with_faces_only]"
      ],
      "metadata": {
        "id": "lIMjERYC_IWp"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_eyebrows(img):\n",
        "  eyes_roi = [img[y:y+h, x:x+w] for x, y, w, h in eye_cascade.detectMultiScale(img)]\n",
        "\n",
        "  # Define colors to look for (brown shades)\n",
        "  lower = np.array([28, 11, 5], dtype=np.uint8)\n",
        "  upper = np.array([50, 20, 20], dtype=np.uint8)\n",
        "\n",
        "  response = []\n",
        "\n",
        "  # Analyze color values in each ROI\n",
        "  for i, roi in enumerate(eyes_roi):\n",
        "      # Convert ROI to HSV color space\n",
        "      hsv = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)\n",
        "      \n",
        "      # Threshold image to extract brown pixels\n",
        "      mask = cv2.inRange(hsv, lower, upper)\n",
        "      \n",
        "      # Find contours of brown regions\n",
        "      contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
        "      \n",
        "      # Draw contours on original image\n",
        "      for contour in contours:\n",
        "          x, y, w, h = cv2.boundingRect(contour)\n",
        "          cv2.rectangle(roi, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
        "\n",
        "      response.append(contours)\n",
        "    \n",
        "  return response\n",
        "\n",
        "eyebrow_contours = [extract_eyebrows(img) for img in imgs_with_faces_only]"
      ],
      "metadata": {
        "id": "Wrn9Qjc73MqG"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extracting eye corners\n",
        "\n",
        "Reason to extract eye corners is to measure the position and movement of the eyes as eye tracking relies on detecting eye landmarks such as the corners to determine where a person is looking and how their gaze is moving."
      ],
      "metadata": {
        "id": "ePMu2Ff1lmmh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_eye_corners(img_gray):\n",
        "    # Iterate through the detected faces\n",
        "    all_corners = []\n",
        "\n",
        "    # Detect eyes in the ROI\n",
        "    eyes = eye_cascade.detectMultiScale(img_gray, scaleFactor=1.2, minNeighbors=5)\n",
        "\n",
        "    # Iterate through the detected eyes\n",
        "    for (ex, ey, ew, eh) in eyes:\n",
        "        # Extract eye region and apply adaptive thresholding\n",
        "        eye_gray = img_gray[ey - 15:ey+eh + 15, ex- 15:ex+ew + 15]\n",
        "        eye_thresh = cv2.adaptiveThreshold(eye_gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 9, 3)\n",
        "\n",
        "        # Apply Harris corner detection\n",
        "        gray_corners = cv2.cornerHarris(eye_thresh, 2, 3, 0.04)\n",
        "        corners = cv2.goodFeaturesToTrack(gray_corners, 2, 0.005, 25, blockSize=45)\n",
        "        corners = np.int0(corners)\n",
        "\n",
        "        # Append the detected corners to the list of all corners\n",
        "        all_corners.append(corners)\n",
        "\n",
        "    return all_corners\n",
        "\n",
        "\n",
        "eye_corners = [detect_eye_corners(img) for img in imgs_gray_with_faces_only]"
      ],
      "metadata": {
        "id": "MRDAvl30tfMx"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extracting pupil size\n",
        "\n",
        "Reason to extract the size of the pupil is that it does changes in response to changes in lighting and focus. We can use contour detection or Hough circle detection to detect and extract pupil\n",
        "\n",
        "1.   Detect the contour of the pupil.\n",
        "2.   Find the contour with the maximum area, as it is likely to be the pupil\n"
      ],
      "metadata": {
        "id": "VCvOzsYw0lbS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_pupil(img_gray):\n",
        "    all_pupil_sizes = []\n",
        "\n",
        "    eyes = eye_cascade.detectMultiScale(img_gray, scaleFactor=1.2, minNeighbors=5)\n",
        "\n",
        "    for (ex, ey, ew, eh) in eyes:\n",
        "      eyes_roi = img_gray[ey:ey+eh, ex:ex+ew]\n",
        "      # Find the contours of the pupil\n",
        "      contours, hierarchy = cv2.findContours(eyes_roi, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "      # Find the contour with the maximum area, which is likely the pupil\n",
        "      max_area = 0\n",
        "      max_contour = None\n",
        "      for contour in contours:\n",
        "          area = cv2.contourArea(contour)\n",
        "          if area > max_area:\n",
        "              max_area = area\n",
        "              max_contour = contour\n",
        "\n",
        "      # Draw a circle around the pupil\n",
        "      (x, y), radius = cv2.minEnclosingCircle(max_contour)\n",
        "\n",
        "      # Return the coordinates and radius of the pupil\n",
        "      all_pupil_sizes.append(((int(x), int(y)), int(radius)))\n",
        "\n",
        "    return all_pupil_sizes\n",
        "\n",
        "pupil_sizes = [extract_pupil(img) for img in imgs_gray_with_faces_only]"
      ],
      "metadata": {
        "id": "z5nTSjkiy-iv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Processing"
      ],
      "metadata": {
        "id": "-potdiVg-VrC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We combine all the features we extracted into a feature matrix and perform normalisation and data spliting before passing it to the algorithm to train"
      ],
      "metadata": {
        "id": "Cyr-XNG8NFab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def append_features_to_matrix(feature_matrix, features):\n",
        "    \"\"\"\n",
        "    Helper method to split and reshape input arrays and append them to a feature matrix.\n",
        "    :param matrix: Feature matrix to append features to.\n",
        "    :param features: Features to append to the matrix.\n",
        "    :return: Updated feature matrix.\n",
        "    \"\"\"\n",
        "    # Split x, y coordinates from features\n",
        "    x, y = features[0][0], features[0][1]\n",
        "\n",
        "    # Reshape x and y coordinates and stack them together\n",
        "    feature = np.array([x, y, radius]).reshape(1, -1)\n",
        "\n",
        "    # Append reshaped features to the feature matrix\n",
        "    feature_matrix = np.append(feature_matrix, feature)\n",
        "    return feature_matrix\n",
        "\n",
        "# Available variables: pupil_sizes, eye_corners, eyebrow_contours, faces, eye positions\n",
        "\n",
        "# Create empty feature matrix\n",
        "feature_matrix = np.empty((0, 0))\n",
        "pupil_sizes_2d = np.array([pupil_sizes[0][0], pupil_sizes[0][1], pupil_sizes[1]]).reshape(1, -1)\n",
        "feature_matrix = np.concatenate((feature_matrix, pupil_sizes_2d), axis=1)\n",
        "\n",
        "\n",
        "feature_matrix = append_features_to_matrix(feature_matrix, eye_corners)\n",
        "feature_matrix = append_features_to_matrix(feature_matrix, eyebrow_contours)\n",
        "feature_matrix = append_features_to_matrix(feature_matrix, faces)\n",
        "feature_matrix = append_features_to_matrix(feature_matrix, eye_positions)\n",
        "\n",
        "\n",
        "X = feature_matrix  # feature matrix (pupil sizes, eye corners, eyebrow contours, faces, and eye positions)\n",
        "y = ...  # label vector (gaze direction)\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalize the data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zsCnZFk1-Uvl",
        "outputId": "31b0b12a-39d1-4485-bba2-2aefd083e79c"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-92-131ce79cfa52>:12: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  feature = np.array([x, y, radius]).reshape(1, -1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Training"
      ],
      "metadata": {
        "id": "d1v9jw6R9FQL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train an XGBoost model\n",
        "\n",
        "params = {\n",
        "    'objective': 'multi:softmax',\n",
        "    'num_class': 9,  # assuming 9 eye gazing directions\n",
        "    'max_depth': 3,\n",
        "    'learning_rate': 0.1,\n",
        "    'n_estimators': 100,\n",
        "    'gamma': 0,\n",
        "    'min_child_weight': 1,\n",
        "    'subsample': 0.8,\n",
        "    'colsample_bytree': 0.8,\n",
        "    'reg_alpha': 0,\n",
        "    'reg_lambda': 1,\n",
        "    'seed': 42\n",
        "}\n",
        "\n",
        "model = xgb.XGBClassifier(**params)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print('Accuracy: {:.2f}%'.format(accuracy * 100))"
      ],
      "metadata": {
        "id": "Na6W2Iik7snu"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Understand Feature Importance and Contribution to prediction"
      ],
      "metadata": {
        "id": "AGRIxcsWBrRz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "\n",
        "# Load the dataset\n",
        "X, y = shap.datasets.iris()\n",
        "\n",
        "# Calculate the SHAP values\n",
        "explainer = shap.Explainer(model)\n",
        "shap_values = explainer(X)\n",
        "\n",
        "# Visualize the SHAP values\n",
        "shap.summary_plot(shap_values, X)\n",
        "\n",
        "# Print the feature importance\n",
        "feature_importance = model.feature_importances_\n",
        "feature_names = ['pupil_size', 'eye_corner_x', 'eye_corner_y', 'eyebrow_contour_x', 'eyebrow_contour_y', 'face_x', 'face_y', 'eye_x', 'eye_y']\n",
        "for name, importance in zip(feature_names, feature_importance):\n",
        "    print('{}: {:.2f}%'.format(name, importance * 100))"
      ],
      "metadata": {
        "id": "1SWb-3kCA3jG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eWGztT7AA7FR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}